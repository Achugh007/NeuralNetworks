import os
import requests
import numpy as np
import nltk
from nltk.corpus import stopwords
from gensim.models import KeyedVectors
from scipy import spatial

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Directory and file paths (ensure this path is correct!)
labfiles_dir = r'd:\Labfiles'  # Or the correct directory if it's different
model_file = os.path.join(labfiles_dir, 'glove.6B.100d.txt.word2vec')

# Check if the file exists, download if not
if not os.path.exists(model_file):
    # ... (download and extraction logic as before)

# Load the GloVe model (check for typos in the filename)
model = KeyedVectors.load_word2vec_format(model_file, binary=False)

# Load the GloVe model (adjust path if needed)
model_file = r'd:\Labfiles\glove.6B.100d.txt.word2vec' 
model = KeyedVectors.load_word2vec_format(model_file, binary=False)

def preprocess(text):
    """Performs basic text preprocessing (tokenization, stopword removal)."""
    tokens = nltk.word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    filtered_words = [w.lower() for w in tokens if w.lower() not in stop_words]
    return filtered_words

def calculate_similarity(text1, text2, model):
    """Calculates cosine similarity between two preprocessed texts."""
    embedding1 = np.mean([model[word] for word in text1 if word in model.vocab], axis=0)
    embedding2 = np.mean([model[word] for word in text2 if word in model.vocab], axis=0)
    return 1 - spatial.distance.cosine(embedding1, embedding2)


# Example usage
text1 = "This is a sample sentence."
text2 = "This is another sentence for comparison."

# Preprocess the texts
processed_text1 = preprocess(text1)
processed_text2 = preprocess(text2)

similarity_score = calculate_similarity(processed_text1, processed_text2, model)
print("Similarity Score:", similarity_score) 
