import nltk
from gensim.models import Word2Vec

# Download NLTK's 'punkt' tokenizer data (if not already downloaded)
nltk.download('punkt')

# Sample text corpus (replace with your own data)
sentences = [
    "This is a sample sentence",
    "Word2Vec learns word embeddings",
    "Embeddings capture semantic relationships"
]

# 1. Tokenize the Sentences
tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]

# 2. Train the Word2Vec Model (with Lowercasing)
model = Word2Vec(
    sentences=[
        [word.lower() for word in sentence] for sentence in tokenized_sentences
    ],
    vector_size=100,  # Dimensionality of word vectors
    window=5,         # Context window size
    min_count=1,      # Minimum word frequency
    workers=4,        # Number of threads for training
)

# 3. Explore Word Vectors
words = list(model.wv.key_to_index)

print("\nWord Embeddings:")
for word in words[:5]:  # Print the first 5 word vectors
    print(f"{word}: {model.wv[word]}")
