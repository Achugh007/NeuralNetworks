import nltk
from gensim.models import Word2Vec

# Download NLTK's 'punkt' tokenizer data (if not already downloaded)
nltk.download('punkt')

# Sample text corpus (each sentence is already a list of words)
sentences = [
    ["This", "is", "a", "sample", "sentence"],
    ["Word2Vec", "learns", "word", "embeddings"],
    ["Embeddings", "capture", "semantic", "relationships"]
]

# Train the Word2Vec Model (with Lowercasing)
model = Word2Vec(
    sentences=[
        [word.lower() for word in sentence] for sentence in sentences
    ],
    vector_size=100,  # Dimensionality of word vectors
    window=5,         # Context window size
    min_count=1,      # Minimum word frequency
    workers=4,        # Number of threads for training
)

# Explore Word Vectors
words = list(model.wv.key_to_index)

print("\nWord Embeddings:")
for word in words[:5]:  # Print the first 5 word vectors
    print(f"{word}: {model.wv[word]}")
