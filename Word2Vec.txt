import nltk

# Download NLTK's 'punkt' tokenizer data (if not already downloaded)
nltk.download('punkt')

from gensim.models import Word2Vec

# Sample text corpus (replace with your own data)
sentences = [
    ["This", "is", "a", "sample", "sentence"],
    ["Word2Vec", "learns", "word", "embeddings"],
    ["Embeddings", "capture", "semantic", "relationships"]
]

# 1. Tokenize the Sentences
tokenized_sentences = [nltk.word_tokenize(sentence.lower()) for sentence in sentences]

# 2. Train the Word2Vec Model
# Hyperparameters (adjust as needed)
vector_size = 100  # Dimensionality of word vectors
window = 5        # Context window size
min_count = 1     # Minimum word frequency
workers = 4       # Number of threads for training

model = Word2Vec(tokenized_sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers)

# 3. Explore Word Vectors
words = list(model.wv.key_to_index)

print("\nWord Embeddings:")
for word in words[:5]:  # Print the first 5 word vectors
    print(f"{word}: {model.wv[word]}")
